# Web Scraper By F1shh

import requests
import threading
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import sys
# Initial URL

# Takes input in the form of Domain, URL, Depth
#URL needs to include HTTPS "https://example.com"
# The domain in scope should not include https

url = sys.argv[2]
scope = sys.argv[1]

# Adds one to depth
depthLimit = int(sys.argv[3])

# Create an empty list to store the visited URLs
SiteGrabbed = []

def get_links(url, depth):
    # Add the URL to the list of visited URLs

    try:
        currentPage = requests.get(url)
    except:
        return
    # only appends the page if its accessable
    SiteGrabbed.append(url)
    soupParsed = BeautifulSoup(currentPage.content, "html.parser")
    links = []
    for link in soupParsed.find_all(["a", "script", "link"]):
        # Attempts to see if the link has a href or src
        try:
            foundLink = link.get("href") or link.get("src")
        except:
            return
        # as long as a link is found
        if foundLink is not None:
            # Check to see if it starts with /
            if foundLink[:1] == "/":
                # If it does that means its a link to the current domain, add the domain of the current page
                domain = urlparse(url).netloc
                # Then add the https
                foundLink = "https://" + domain + foundLink
            if "http" and scope in foundLink:
                # Filters out #about and #chapterone which are links to the sections in the page
                if "#" not in foundLink:
                    # Filters out email addresses
                    if "mailto" not in foundLink:
                        links.append(foundLink)
    # Recursion Case
    if depth < depthLimit:
        for link in links:
            # Checks to see if the link was visited already
            if link not in SiteGrabbed:
                print(link)
                # Option to not use threading
                #get_links(link, depth + 1)
                # Recurse in a new thread
                threading.Thread(target=get_links, args=(link, depth + 1)).start()
    return links

# calls get_links with a starting depth of 0
get_links(url,0)
print(len(SiteGrabbed))
with open("output.txt", "w") as output:
    for elm in SiteGrabbed:
        output.writelines(elm + "\n")

